{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy in Finding a Business Location\n",
    "\n",
    "To find the best business location, start by analyzing your target market, researching potential areas, and considering factors like accessibility, infrastructure, and zoning regulations.\n",
    "Here's a more detailed strategy:  \n",
    "\n",
    "##  1. Define Your Needs and Goals: \n",
    "- **Target Market:** Identify your ideal customer base and where they are located\n",
    "    - For definition see [this](https://www.investopedia.com/terms/t/target-market.asp#:~:text=Demographic%3A%20These%20are%20the%20main,in%20the%20era%20of%20globalization.)\n",
    "- **Business Type:** Determine if your business requires \n",
    "    - high foot traffic\n",
    "    - proximity to suppliers\n",
    "    - access_specific infrastructure (e.g., highways, rail yards)\n",
    "- **Budget:** Establish a realistic budget for \n",
    "    - rent\n",
    "    - utilities\n",
    "    - other location-related costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Market Analysis will do the following:\n",
    "* **Market Analysis** – Provides insights into customer aggregate data, market trends, and competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Laundromat\n",
    "target_market = {\n",
    "    \"description\": \"Laundromats serve individuals and families who lack in-unit laundry facilities, offering self-service washing and drying options. They cater to a diverse clientele, including renters, students, and busy professionals.\",\n",
    "    \"demographics\": {\n",
    "        \"age_range\": \"18–55\", \n",
    "        \"gender_distribution\": {\"female\": 60, \"male\": 40},\n",
    "        \"household_income\": {\"median\": 28_000, \"range\": \"15,000–60,000\"},\n",
    "        \"primary_residence\": [\"renters\", \"apartment dwellers\", \"students\", \"dual-income households\", \"middle-aged individuals\", \"\"],\n",
    "        \"interpetation_and_strategic_insight\": \"The laundromat clientele predominantly comprises renters and apartment dwellers, with a significant portion being students and dual-income households. Understanding this demographic is crucial for tailoring services and marketing efforts.\"\n",
    "    },\n",
    "    \"usage_patterns\": {\n",
    "        \"frequency\": {\"weekly\": 60, \"weekend_usage\": 70},\n",
    "        \"average_spend_per_visit\": {\"low_end\": 10, \"high_end\": 20},\n",
    "        \"average_time_spent_per_visit\": {\"min\": 60, \"max\": 90},\n",
    "        \"interpetation_and_strategic_insight\": \"Most customers visit laundromats weekly, with peak usage occurring on weekends, particularly Saturdays. Offering promotions or extended hours during these times can attract more customers.\"\n",
    "    },\n",
    "    \"kpis_and_metrics_to_track\": {\n",
    "        \"operational_efficiency_metrics\": {\n",
    "            \"machine_utilization_rate\": {\n",
    "            \"description\": \"Measures the percentage of time machines are in use versus their total available time.\",\n",
    "            \"calculated_metric\": \"machine_downtime_per_load\"\n",
    "            },\n",
    "            \"utility_costs_per_load\": {\n",
    "            \"description\": \"Evaluates the utility cost (water, electricity, gas) per load of laundry.\",\n",
    "            \"calculated_metric\": \"cost_savings_from_efficiency_measures\"\n",
    "            },\n",
    "            \"machine_downtime_percentage\": {\n",
    "            \"description\": \"Measures the percentage of time machines are inactive due to issues like maintenance or repair.\",\n",
    "            \"calculated_metric\": \"revenue_loss_due_to_downtime\"\n",
    "            }\n",
    "        },\n",
    "        \"financial_performance_metrics\": {\n",
    "            \"gross_income_and_profitability\": {\n",
    "            \"description\": \"Monitors total revenue and net profit after expenses to assess financial health.\",\n",
    "            \"calculated_metric\": \"profitability_ratio\"\n",
    "            },\n",
    "            \"average_order_value\": {\n",
    "            \"description\": \"Tracks the average amount spent per customer transaction.\",\n",
    "            \"calculated_metric\": \"aov_growth_rate\"\n",
    "            },\n",
    "            \"cost_per_pound_of_laundry\": {\n",
    "            \"description\": \"Measures average cost to process each pound of laundry.\",\n",
    "            \"calculated_metric\": \"cost_efficiency_index\"\n",
    "            },\n",
    "            \"profit_margin\": {\n",
    "            \"description\": \"Indicates what percentage of revenue remains after all expenses.\",\n",
    "            \"calculated_metric\": \"margin_improvement_percentage\"\n",
    "            }\n",
    "        },\n",
    "        \"customer_satisfaction_and_retention_metrics\": {\n",
    "            \"customer_retention_rate\": {\n",
    "            \"description\": \"Measures the percentage of customers who return over a given time period.\"\n",
    "            },\n",
    "            \"net_promoter_score\": {\n",
    "            \"description\": \"Assesses customer loyalty based on likelihood to recommend the laundromat.\"\n",
    "            },\n",
    "            \"customer_feedback_and_complaints\": {\n",
    "            \"description\": \"Tracks feedback and complaints to identify customer needs and improve service quality.\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"location_proximity\": {\n",
    "        \"within_one_mile\": 87\n",
    "    },\n",
    "    \"revenue_statistics\": {\n",
    "        \"average_revenue_per_laundromat\": 142_000,\n",
    "        \"average_revenue_per_machine\": 1_500,\n",
    "        \"profit_margin\": {\"estimated_low\": 15, \"estimated_high\": 30},\n",
    "        \"startup_cost_range\": {\"low\": 200_000, \"high\": 500_000}\n",
    "    },\n",
    "    \"sources\": [\n",
    "        {\"name\": \"Skyrocket BPO Laundromat Industry Analysis\", \n",
    "         \"url\": \"https://www.skyrocketbpo.com/laundromat-industry-analysis\"},\n",
    "         \n",
    "        {\"name\": \"Crucial Stats Every Laundromat Owner Must Know Before Starting or Investing\", \n",
    "         \"url\": \"https://www.turnsapp.com/blog/key-statistics-every-laundromat-owner-should-know-before-starting-or-investing\"},\n",
    "\n",
    "        {\"name\": \"How Laundromats Combat Period Poverty and Period Stigma\", \n",
    "         \"url\": \"https://endometriosis.net/living/laundromats\"},\n",
    "\n",
    "        {\"name\": \"Essential Laundromat Equipment for Starting Your Business\", \n",
    "         \"url\": \"https://metrobi.com/blog/essential-laundromat-equipment-for-your-business\"},\n",
    "\n",
    "        {\"name\": \"KPI to keep track of\", \n",
    "         \"url\": \"https://www.flexwasher.com/laundromat-kpis-and-metrics\"}\n",
    "    ],\n",
    "    \"anecdote\": [\n",
    "        {\"name\": \"We bought a laundromat and its all about the numbers\", \n",
    "         \"url\": \"https://laundromats101.com/2019/01/we-bought-a-laundromat-and-its-all-about-the-numbers\"}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Business Type Considerations\n",
    "\n",
    "- **High Foot Traffic**:  \n",
    "  Required — areas near apartment complexes, universities, and public transport hubs are ideal.\n",
    "\n",
    "- **Proximity to Suppliers**:  \n",
    "  Moderately Important — access to maintenance services and detergent vendors is helpful but not critical.\n",
    "\n",
    "- **Access to Infrastructure (e.g., highways, rail yards)**:  \n",
    "  Not Required — but local road access and available parking are very important for customer convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Budget\n",
    "- Here’s the full laundromat startup and operating budget for Pennsylvania, formatted as a detailed Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laundromat_budget_pa = {\n",
    "    \"startup_costs\": {\n",
    "        \"lease_deposit\": {\n",
    "            \"estimated\": [10000, 30000],\n",
    "            \"description\": \"Typically 3–6 months' rent upfront.\"\n",
    "        },\n",
    "        \"commercial_equipment\": {\n",
    "            \"estimated\": [40000, 260000],\n",
    "            \"description\": \"Includes washers, dryers, folding tables, POS systems.\"\n",
    "        },\n",
    "        \"renovations_buildout\": {\n",
    "            \"estimated\": [20000, 50000],\n",
    "            \"description\": \"Plumbing, electrical, interior layout, flooring.\"\n",
    "        },\n",
    "        \"licenses_permits\": {\n",
    "            \"estimated\": [500, 2000],\n",
    "            \"description\": \"Business license, health inspections, local permits.\"\n",
    "        },\n",
    "        \"marketing_signage\": {\n",
    "            \"estimated\": [2000, 5000],\n",
    "            \"description\": \"Initial promotional material, grand opening, branding.\"\n",
    "        },\n",
    "        \"insurance\": {\n",
    "            \"estimated_annual\": [2000, 3000],\n",
    "            \"description\": \"Property, liability, workers’ compensation.\"\n",
    "        },\n",
    "        \"contingency_fund\": {\n",
    "            \"estimated_percent_of_total_investment\": [5, 10],\n",
    "            \"description\": \"Reserved for unexpected expenses.\"\n",
    "        },\n",
    "        \"total_startup_estimate\": [101000, 420000]\n",
    "    },\n",
    "    \"monthly_operating_costs\": {\n",
    "        \"lease_or_mortgage\": {\n",
    "            \"estimated\": [5000, 10000],\n",
    "            \"description\": \"Rental or financing costs for commercial space.\"\n",
    "        },\n",
    "        \"utilities\": {\n",
    "            \"estimated\": [4500, 9000],\n",
    "            \"description\": \"Water, electricity, gas — laundromats are utility-heavy.\"\n",
    "        },\n",
    "        \"employee_wages\": {\n",
    "            \"estimated\": [2000, 3500],\n",
    "            \"description\": \"Attendants, shift managers, etc.\"\n",
    "        },\n",
    "        \"maintenance_repairs\": {\n",
    "            \"estimated\": [500, 5000],\n",
    "            \"description\": \"Regular and emergency servicing of machines and infrastructure.\"\n",
    "        },\n",
    "        \"supplies_inventory\": {\n",
    "            \"estimated\": [1000, 2000],\n",
    "            \"description\": \"Detergents, vending machine items, cleaning products.\"\n",
    "        },\n",
    "        \"insurance\": {\n",
    "            \"monthly_equivalent\": [125, 250],\n",
    "            \"description\": \"Liability and property insurance (monthly portion).\"\n",
    "        },\n",
    "        \"total_monthly_estimate\": [13500, 29500]\n",
    "    },\n",
    "    \"additional_info\": {\n",
    "        \"revenue_potential\": {\n",
    "            \"annual\": [30000, 300000],\n",
    "            \"profit_margin_percent\": [20, 35]\n",
    "        },\n",
    "        \"financing_options\": [\n",
    "            \"SBA loans\",\n",
    "            \"Equipment financing\",\n",
    "            \"Business line of credit\"\n",
    "        ],\n",
    "        \"location_factors\": [\n",
    "            \"Higher rent in urban areas balanced by higher foot traffic and customer volume\"\n",
    "        ]\n",
    "    },\n",
    "    \"sources\": [\n",
    "        {\n",
    "            \"name\": \"Upmetrics\",\n",
    "            \"url\": \"https://upmetrics.co/startup-costs/laundromat\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Consulting Times\",\n",
    "            \"url\": \"https://consultingtimes.com/how-much-does-it-cost-to-start-a-laundromat-business\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"The Pricer\",\n",
    "            \"url\": \"https://www.thepricer.org/cost-to-start-a-laundromat\"\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### 1. Demographic Compatibility\n",
    "- Target neighborhoods with:\n",
    "  - High concentration of renters\n",
    "  - Low-to-moderate income households\n",
    "  - Students, seniors, or young professionals\n",
    "- Use tools like **U.S. Census Data** or **local property databases**.\n",
    "\n",
    "### 2. Parking and Accessibility\n",
    "- Ensure **ample off-street parking**\n",
    "- **ADA compliance** (ramps, wide entryways) is both legally and commercially wise\n",
    "\n",
    "### 3. Visibility & Signage\n",
    "- Ideal locations:\n",
    "  - Street-facing\n",
    "  - Corner lots\n",
    "  - High pedestrian zones\n",
    "- Invest in **well-lit, branded signage**\n",
    "\n",
    "### 4. Competitor Proximity\n",
    "- Avoid clustering unless offering **superior services**:\n",
    "  - Wash-and-fold\n",
    "  - Free Wi-Fi\n",
    "  - Loyalty programs\n",
    "- Check **Google Maps** or **Yelp** for competitor density\n",
    "\n",
    "### 5. Safety & Security\n",
    "- Choose well-lit, low-crime areas\n",
    "- Install visible **security cameras** to promote customer trust\n",
    "\n",
    "### 6. Zoning and Utilities\n",
    "- Confirm **zoning allows commercial laundry services**\n",
    "- Ensure access to:\n",
    "  - **3-phase power**\n",
    "  - **High water pressure**\n",
    "  - **Proper drainage systems**\n",
    "\n",
    "### 7. Neighboring Businesses\n",
    "- Favor adjacency to:\n",
    "  - Coffee shops\n",
    "  - Takeout restaurants\n",
    "  - Convenience stores  \n",
    "These increase **customer dwell time** and cross-business synergy.\n",
    "\n",
    "### 8. Budget Considerations\n",
    "- Total startup costs ranging from **<span>&#36;</span>101,000 to <span>&#36;</span>420,000**, primarily driven by equipment and renovation expenses. \n",
    "- Monthly operating costs fall between **<span>&#36;</span>13,500 and <span>&#36;</span>29,500**, reflecting the utility-intensive and service-oriented nature of the business. \n",
    "- While the annual revenue potential varies widely—from **<span>&#36;</span>30,000 to <span>&#36;</span>300,000**\n",
    "- Profit margins of **20% to 35%** suggest a solid opportunity for profitability, especially in high-traffic urban locations. \n",
    "- Strategic financing and thorough planning are essential to manage costs and maximize returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Potential Locations:\n",
    "- **Demographics:** Research the demographics of potential areas to ensure they align with your target market\n",
    "- **Competition:** Analyze the competitive landscape in each area to understand the level of competition\n",
    "- **Traffic:** Consider traffic patterns and parking availability, especially if foot traffic is important\n",
    "- **Psychographic Information (optional):** Go beyond basic demographics to explore customer lifestyles, values, and habits. \n",
    "    - For instance, are there large groups of environmentally conscious consumers in your area who might appreciate eco-friendly laundry solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate & Analyze Customers and Market with **Census Business Builder**\n",
    "- [Video How-To](https://www.census.gov/data/academy/data-gems/2023/locate-analyze-customers-market-with-cbb.html)\n",
    "- [Census Business Builder](https://cbb.census.gov/cbb/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Research on Businesses, Markets, Demographics etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tools.eval_measures import aic\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from us import states\n",
    "import requests\n",
    "import zipcodes\n",
    "import addfips\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "import certifi\n",
    "\n",
    "# Created modules\n",
    "from ipython_config import CENSUS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coin-Operated Laundries and Drycleaners\n",
    "NAICS = '812310'\n",
    "YEAR = '2023'\n",
    "\n",
    "# Vandergrift, PA\n",
    "# ZIPCODE = '15690'\n",
    "\n",
    "# Pittsburgh, PA\n",
    "ZIPCODE = '15120'\n",
    "\n",
    "state = states.PA\n",
    "STATEFIPS = state.fips\n",
    "STATENAME = state.name\n",
    "\n",
    "result = zipcodes.matching(ZIPCODE)[0]\n",
    "COUNTYNAME = result['county']\n",
    "\n",
    "af = addfips.AddFIPS()\n",
    "\n",
    "# Get FIPS code for a single county\n",
    "COUNTYFIP = af.get_county_fips(county=COUNTYNAME, state=STATENAME)\n",
    "COUNTYCODE = COUNTYFIP[2:]\n",
    "\n",
    "print(f\"\"\"\n",
    "Initial:\n",
    "NAICS: {NAICS}\n",
    "ZIPCODE: {ZIPCODE}\n",
    "STATEFIPS: {STATEFIPS}\n",
    "STATENAME: {STATENAME}\n",
    "COUNTYNAME: {COUNTYNAME} \n",
    "COUNTYFIP: {COUNTYFIP}  \n",
    "COUNTYCODE: {COUNTYCODE}\n",
    "\"\"\")\n",
    "print(\"Secondary:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.replace('_', '').upper()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "os.environ['PROJ_LIB'] = '/opt/anaconda3/share/proj'\n",
    "\n",
    "# Ensure data directory exists\n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Construct file path\n",
    "tract_file_location = os.path.join(data_dir, f\"{STATEFIPS}_{STATENAME}_tracts_{YEAR}.zip\")\n",
    "\n",
    "with zipfile.ZipFile(tract_file_location, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(data_dir, \"extracted\"))\n",
    "\n",
    "extracted_path = os.path.join(data_dir, \"extracted\")\n",
    "gdf = gpd.read_file(extracted_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API's For Business Statistics\n",
    "- Nonemployer Statistics\n",
    "- Business Patterns County Business Patterns\n",
    "- Economic Census\n",
    "- American Community Survey 1-Year Data (2005-2023)\n",
    "- American Community Survey 5-Year Data (2009-2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exist(url):\n",
    "    filename = '_'.join(url.split('/')[5:])\n",
    "    file_path_os = f'data/{filename}'\n",
    "\n",
    "    res = {'exist': False,\n",
    "            'file_path': file_path_os}\n",
    "    \n",
    "    if os.path.exists(file_path_os):\n",
    "        res['exist'] = True\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def download_census_variables(url):\n",
    "    \"\"\"\n",
    "    Downloads Census variable metadata from a given URL and saves it as a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL to fetch the JSON data from.\n",
    "    - output_path (str): Local path where the JSON file will be saved.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If the request fails or file cannot be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = check_file_exist(url)\n",
    "    filename_exist = filename['exist']\n",
    "    filename_path = filename['file_path']\n",
    "\n",
    "    try:\n",
    "        if filename_exist:\n",
    "            with open(filename_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"{filename_path} exists and successfully downloaded\")\n",
    "        else:\n",
    "            data = requests.get(url)\n",
    "            data.raise_for_status()\n",
    "            data = data.json()\n",
    "\n",
    "            print(f\"Successfully downloaded {url}\")\n",
    "            with open(filename_path, 'w') as f:\n",
    "                json.dump(data, f)  \n",
    "            print(f\"Successfully saved {url} to {filename_path}\")\n",
    "      \n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Request error: {req_err}\")\n",
    "    except OSError as file_err:\n",
    "        print(f\"File error: {file_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An unexpected error occurred: {err}\")\n",
    "\n",
    "\n",
    "def create_data_dictionary(url, relevant_variables):\n",
    "    data = download_census_variables(url)\n",
    "    variables_df = pd.DataFrame(data['variables']).T.reset_index()\n",
    "\n",
    "    relevant_cols = relevant_variables.split(',')\n",
    "\n",
    "    filtered_df = variables_df[variables_df['index'].isin(relevant_cols)][['index', 'label', 'concept']]\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        var_name = row['index']\n",
    "        description = f\"{row['label']} {row['concept']}\"\n",
    "        print(f\"'{var_name}': '{description}',\")\n",
    "\n",
    "\n",
    "def search_variables_in_data_dictionary(url, word):\n",
    "    word = word.lower()\n",
    "    data = download_census_variables(url)\n",
    "    variables_df = pd.DataFrame(data['variables']).T.reset_index().astype(str)\n",
    "\n",
    "    variables_df['label_concept'] = (variables_df['label']+' '+variables_df['concept']).str.lower()\n",
    "\n",
    "    filtered_df = variables_df[variables_df['label_concept'].str.contains(word)][['index','label', 'concept']]\n",
    "\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        var_name = row['index']\n",
    "        description = f\"{row['label']} {row['concept']}\"\n",
    "        print(f\"'{var_name}': '{description}',\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "url = \"https://api.census.gov/data/2023/acs/acs5/variables.json\"\n",
    "relevant_variables = ','.join(['C17002_002E', 'C17002_003E', 'B01003_001E'])\n",
    "word = 'one race'\n",
    "\n",
    "create_data_dictionary(url, relevant_variables)\n",
    "search_variables_in_data_dictionary(url, word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.census.gov/data\"\n",
    "\n",
    "# Nonemployer Statistics\n",
    "nonemp_params = {\n",
    "    'variables' : \"\",\n",
    "    'geography': f\"county:{COUNTYCODE}&in=state:{STATEFIPS}\",\n",
    "    'api_key': CENSUS_KEY,\n",
    "    'dataset_base': 'nonemp',\n",
    "    'year': ''\n",
    "}\n",
    "\n",
    "# Business Patterns County Business Patterns\n",
    "cbp_params = {\n",
    "    'variables' : \"\",\n",
    "    'geography': f\"county:{COUNTYCODE}&in=state:{STATEFIPS}\",\n",
    "    'api_key': CENSUS_KEY,\n",
    "    'dataset_base': 'cbp',\n",
    "    'year': ''\n",
    "}\n",
    "\n",
    "# Economic Census\n",
    "ecnbasic_params = {\n",
    "    'variables' : \"\",\n",
    "    'geography': f\"county:{COUNTYCODE}&in=state:{STATEFIPS}\",\n",
    "    'api_key': CENSUS_KEY,\n",
    "    'dataset_base': 'ecnbasic',\n",
    "    'year': ''\n",
    "}\n",
    "\n",
    "# ACS 1-Year Estimates\n",
    "acs1_params = {\n",
    "    'variables' : \"\",\n",
    "    'geography': f\"county:{COUNTYCODE}&in=state:{STATEFIPS}\",\n",
    "    'api_key': CENSUS_KEY,\n",
    "    'dataset_base': 'acs/acs1',\n",
    "    'year': ''\n",
    "}\n",
    "\n",
    "# ACS 5-Year Estimates\n",
    "acs5_params = {\n",
    "    'variables' : \"\",\n",
    "    'geography': f\"tract:*&in=state:{STATEFIPS}&in=county:{COUNTYCODE}\",\n",
    "    'api_key': CENSUS_KEY,\n",
    "    'dataset_base': 'acs/acs5',\n",
    "    'year': ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(abs_params):\n",
    "    \"\"\"\n",
    "    Retrieve data from the U.S. Census Bureau API.\n",
    "\n",
    "    Parameters:\n",
    "    - abs_params (dict): Dictionary with the following keys:\n",
    "        - 'year': (str) Year of the dataset (e.g., \"2020\")\n",
    "        - 'dataset_base': (str) Dataset name (e.g., \"acs/acs5\")\n",
    "        - 'variables': (str) Comma-separated variable names\n",
    "        - 'geography': (str) Geographic level (e.g., \"us:1\")\n",
    "        - 'api_key': (str) Census Bureau API key (optional)\n",
    "\n",
    "    Returns:\n",
    "    - list: Parsed JSON data from the API.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If request fails or required parameters are missing.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.census.gov/data\"\n",
    "\n",
    "    try:\n",
    "        year = abs_params['year']\n",
    "        dataset = abs_params['dataset_base']\n",
    "        variables = abs_params['variables']\n",
    "        geography = abs_params['geography']\n",
    "        api_key = abs_params.get('api_key', '')\n",
    "\n",
    "        url = f\"{base_url}/{year}/{dataset}?get={variables}&for={geography}\"\n",
    "        print(url)\n",
    "        if api_key:\n",
    "            url += f\"&key={api_key}\"\n",
    "\n",
    "        # Safe debug print without exposing API key\n",
    "        print(f\"Requesting Census data for {year}, dataset: {dataset}, geography: {geography}\")\n",
    "\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing required parameter: {e}\")\n",
    "    except requests.RequestException as e:\n",
    "        raise Exception(f\"API request failed: {e}\")\n",
    "\n",
    "\n",
    "def get_all_business_code_table(df, naics_code, naics_column):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to include rows matching all prefix levels of a given NAICS code.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing NAICS data.\n",
    "    - naics_code (str): Full NAICS code to extract prefix matches for.\n",
    "    - naics_column (str): Name of the column in `df` containing NAICS codes.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame with all matching NAICS code prefixes.\n",
    "    \"\"\"\n",
    "    full_list = []\n",
    "    for i in range(1, len(naics_code) + 1):\n",
    "        naics_prefix = naics_code[:i]\n",
    "        matching_rows = df[df[naics_column] == naics_prefix]\n",
    "        if not matching_rows.empty:\n",
    "            full_list.append(matching_rows)\n",
    "\n",
    "    if full_list:\n",
    "        return pd.concat(full_list, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    \n",
    "def combine_and_rename_naics(df_list):\n",
    "    \"\"\"\n",
    "    Combines a list of DataFrames and standardizes NAICS columns.\n",
    "    Handles missing year-specific NAICS columns gracefully.\n",
    "    \n",
    "    Parameters:\n",
    "        df_list (list of pd.DataFrame): List of DataFrames to combine.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and combined DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Safely retrieve NAICS columns if they exist\n",
    "    naics_cols = ['NAICS2022', 'NAICS2017', 'NAICS2012']\n",
    "    naics_label_cols = ['NAICS2022_LABEL', 'NAICS2017_LABEL', 'NAICS2012_LABEL']\n",
    "\n",
    "    # Combine available NAICS codes\n",
    "    df['NAICS'] = None\n",
    "    for col in naics_cols:\n",
    "        if col in df.columns:\n",
    "            df['NAICS'] = df['NAICS'].fillna(df[col])\n",
    "\n",
    "    # Combine available NAICS labels\n",
    "    df['NAICS_LABEL'] = None\n",
    "    for col in naics_label_cols:\n",
    "        if col in df.columns:\n",
    "            df['NAICS_LABEL'] = df['NAICS_LABEL'].fillna(df[col])\n",
    "\n",
    "    # Drop original columns that exist\n",
    "    cols_to_drop = naics_cols + naics_label_cols\n",
    "    df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Nonemployer Statistics (2012 - 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonemp_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{nonemp_params['dataset_base']}.csv\"\n",
    "if os.path.exists(nonemp_file_path):\n",
    "    nonemp = pd.read_csv(nonemp_file_path)\n",
    "else: \n",
    "    nonemp_all_years = []\n",
    "    for i in range(2012, 2026):\n",
    "        if i >= 2022:\n",
    "            naics_year = 2022 \n",
    "        elif i >= 2017:\n",
    "            naics_year = 2017\n",
    "        else:\n",
    "            naics_year = 2012\n",
    "\n",
    "        nonemp_params['year'] = i\n",
    "        nonemp_params['variables'] = f'NAME,COUNTY,NAICS{naics_year},NAICS{naics_year}_LABEL,NESTAB,NRCPTOT,YEAR'\n",
    "\n",
    "        try:\n",
    "            nonemp = pd.DataFrame(get_data(nonemp_params))\n",
    "            nonemp.columns = nonemp.iloc[0]\n",
    "            nonemp = nonemp.iloc[1:]\n",
    "            naics_nonemp = get_all_business_code_table(nonemp, NAICS, f'NAICS{naics_year}')\n",
    "            nonemp_all_years.append(naics_nonemp)\n",
    "            \n",
    "            print(f'finished year {i}')\n",
    "            \n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "    \n",
    "    nonemp = combine_and_rename_naics(nonemp_all_years)\n",
    "    nonemp.to_csv(nonemp_file_path, index=False)\n",
    "\n",
    "print(nonemp.info())\n",
    "nonemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_dictionary(f\"{base_url}/2022/{nonemp_params['dataset_base']}/variables.json\", 'NAME,COUNTY,NAICS2022,NAICS2022_LABEL,NESTAB,NRCPTOT,YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Business Patterns County Business Patterns (2012 - 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbp_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{cbp_params['dataset_base']}.csv\"\n",
    "if os.path.exists(cbp_file_path):\n",
    "    cbp = pd.read_csv(cbp_file_path)\n",
    "else: \n",
    "    cbp_all_years = []\n",
    "    for i in range(2012, 2026):\n",
    "        if i >= 2022:\n",
    "            naics_year = 2022 \n",
    "        elif i >= 2017:\n",
    "            naics_year = 2017\n",
    "        else:\n",
    "            naics_year = 2012\n",
    "        cbp_params['year'] = i\n",
    "        cbp_params['variables'] = f\"NAME,COUNTY,EMP,ESTAB,NAICS{naics_year},NAICS{naics_year}_LABEL,PAYANN,PAYANN_N,PAYQTR1,PAYQTR1_N,YEAR\"\n",
    "\n",
    "        try:\n",
    "            cbp = pd.DataFrame(get_data(cbp_params))\n",
    "            cbp.columns = cbp.iloc[0]\n",
    "            cbp = cbp.iloc[1:]\n",
    "            naics_cbp = get_all_business_code_table(cbp, NAICS, f'NAICS{naics_year}')\n",
    "            cbp_all_years.append(naics_cbp)\n",
    "\n",
    "            print(f'finished year {i}')\n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "    cbp = combine_and_rename_naics(cbp_all_years)\n",
    "    cbp.to_csv(cbp_file_path, index=False)\n",
    "\n",
    "print(cbp.info())\n",
    "cbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_dictionary(f\"{base_url}/2022/{cbp_params['dataset_base']}/variables.json\", \n",
    "                       \"NAME,COUNTY,EMP,ESTAB,NAICS2022,NAICS2022_LABEL,PAYANN,PAYANN_N,PAYQTR1,PAYQTR1_N,YEAR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Economic Census (2012 - 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecnbasic_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{ecnbasic_params['dataset_base']}.csv\"\n",
    "if os.path.exists(ecnbasic_file_path):\n",
    "    ecnbasic = pd.read_csv(ecnbasic_file_path)\n",
    "else: \n",
    "    ecnbasic_all_years = []\n",
    "    for i in range(2012, 2026):\n",
    "        if i >= 2022:\n",
    "            naics_year = 2022 \n",
    "        elif i >= 2017:\n",
    "            naics_year = 2017\n",
    "        else:\n",
    "            naics_year = 2012\n",
    "        ecnbasic_params['year'] = i\n",
    "        ecnbasic_params['variables'] = f\"NAME,CBSA,COUNTY,CSA,EMP,ESTAB,NAICS{naics_year},NAICS{naics_year}_LABEL,PAYANN,PAYQTR1,YEAR\"\n",
    "        try:\n",
    "            ecnbasic = pd.DataFrame(get_data(ecnbasic_params))\n",
    "            ecnbasic.columns = ecnbasic.iloc[0]\n",
    "            ecnbasic = ecnbasic.iloc[1:]\n",
    "\n",
    "            naics_ecnbasic = get_all_business_code_table(ecnbasic, NAICS, f'NAICS{naics_year}')\n",
    "            \n",
    "            ecnbasic_all_years.append(naics_ecnbasic)\n",
    "            print(f'finished year {i}')\n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "    ecnbasic = combine_and_rename_naics(ecnbasic_all_years)\n",
    "    ecnbasic.to_csv(ecnbasic_file_path, index=False)\n",
    "\n",
    "print(ecnbasic.info())\n",
    "ecnbasic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_dictionary(f\"{base_url}/2022/{ecnbasic_params['dataset_base']}/variables.json\", \n",
    "                       \"NAME,CBSA,COUNTY,CSA,EMP,ESTAB,NAICS2022,NAICS2022_LABEL,PAYANN,PAYQTR1,YEAR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather American Community Survey 1-Year Data (2012 - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs1_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{acs1_params['dataset_base'].replace('/', '_')}.csv\"\n",
    "if os.path.exists(acs1_file_path):\n",
    "    acs1 = pd.read_csv(acs1_file_path)\n",
    "else: \n",
    "    acs1_all_years = []\n",
    "    for i in range(2012, 2026):\n",
    "        acs1_params['year'] = i\n",
    "        acs1_params['variables'] = \"NAME,B19013_001E,B01003_001E\"\n",
    "        try:\n",
    "            acs1 = pd.DataFrame(get_data(acs1_params))\n",
    "            acs1.columns = acs1.iloc[0]\n",
    "            acs1 = acs1.iloc[1:]     \n",
    "            acs1['year'] = i   \n",
    "            acs1_all_years.append(acs1)\n",
    "\n",
    "            print(f'finished year {i}')\n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "    acs1 = pd.concat(acs1_all_years, ignore_index=True)\n",
    "    acs1.to_csv(acs1_file_path, index=False)\n",
    "\n",
    "print(acs1.info())\n",
    "acs1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_dictionary(f\"{base_url}/2022/{acs1_params['dataset_base']}/variables.json\", \n",
    "                       \"NAME,B19013_001E,B01003_001E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs1_cols_to_rename = {\n",
    "    'B19013_001E': 'Median_Household_Income',\n",
    "    'B01003_001E': 'Population'\n",
    "}\n",
    "\n",
    "new_acs1_cols = list(acs1_cols_to_rename.values())\n",
    "acs1.rename(columns=acs1_cols_to_rename, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Household Income Distribution Analysis** – Assess consumer spending power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs1[new_acs1_cols] = acs1[new_acs1_cols].astype(float)\n",
    "\n",
    "# ESTIMATE: Use a flat 20% tax rate\n",
    "acs1[\"Estimated_Taxes\"] = acs1[\"Median_Household_Income\"] * 0.20\n",
    "acs1[\"Disposable_Income\"] = acs1[\"Median_Household_Income\"] - acs1[\"Estimated_Taxes\"]\n",
    "\n",
    "acs1['Purchasing_Power'] = acs1['Median_Household_Income'] / acs1['Population']\n",
    "\n",
    "# Population growth = (New - Old) / Old * 100\n",
    "acs1.sort_values(by='year', inplace=True)\n",
    "acs1['Pop_Growth'] = acs1['Population'].diff().fillna(0)\n",
    "acs1['Pop_Growth_Rate'] = (acs1['Pop_Growth'] / acs1['Population'] * 100).round(2)\n",
    "acs1['Pop_Growth_Rate %'] = acs1['Pop_Growth_Rate'].astype(str) + ' %'\n",
    "acs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Community and Demographic Analysis** and **Population Density & Growth Trends** – Evaluates purchasing power and population growth and tracks population levels and projected growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot population growth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acs1['year'], acs1['Pop_Growth_Rate'])\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('Population Growth')\n",
    "plt.title('Population Growth by Year (2012 - 2023)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot purchasing power\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acs1['year'], acs1['Purchasing_Power'])\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Purchasing Power (Income per Capita)')\n",
    "plt.title('Purchasing Power by Year (2012 - 2023)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate medians to divide the quadrants\n",
    "median_population = acs1['Population'].median()\n",
    "median_income = acs1['Median_Household_Income'].median()\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(acs1['Population'], acs1['Median_Household_Income'], alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('Population')\n",
    "plt.ylabel('Median Household Income')\n",
    "plt.title('Community Purchasing Power (2012 - 2023)')\n",
    "\n",
    "# Add quadrant lines (dashed)\n",
    "plt.axvline(x=median_population, color='gray', linestyle='--', linewidth=1)\n",
    "plt.axhline(y=median_income, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# Add quadrant labels\n",
    "buffer_x = (acs1['Population'].max() - acs1['Population'].min()) * 0.05\n",
    "buffer_y = (acs1['Median_Household_Income'].max() - acs1['Median_Household_Income'].min()) * 0.05\n",
    "\n",
    "plt.text(median_population + buffer_x, median_income + buffer_y,\n",
    "         'Q1\\nHigh Pop, High Income', fontsize=10, color='darkgreen')\n",
    "plt.text(acs1['Population'].min() + buffer_x, median_income + buffer_y*2,\n",
    "         'Q2\\nLow Pop, High Income', fontsize=10, color='green')\n",
    "plt.text(acs1['Population'].min() + buffer_x, acs1['Median_Household_Income'].min() + buffer_y,\n",
    "         'Q3\\nLow Pop, Low Income', fontsize=10, color='red')\n",
    "plt.text(median_population + buffer_x, acs1['Median_Household_Income'].min() + buffer_y,\n",
    "         'Q4\\nHigh Pop, Low Income', fontsize=10, color='orange')\n",
    "\n",
    "# Add year labels to the scatter points\n",
    "for i in range(len(acs1)):\n",
    "    plt.text(acs1['Population'].iloc[i], acs1['Median_Household_Income'].iloc[i],\n",
    "             str(acs1['year'].iloc[i]), fontsize=10, alpha=0.7, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesForecaster:\n",
    "    def __init__(self, data, time_column, value_column):\n",
    "        \"\"\"\n",
    "        Initialize the forecaster with data, time column, and value column.\n",
    "\n",
    "        :param data: DataFrame containing the time series data.\n",
    "        :param time_column: The name of the time column (e.g., 'year').\n",
    "        :param value_column: The name of the value column (e.g., 'population_growth').\n",
    "        \"\"\"\n",
    "        data[time_column] = pd.to_datetime(data[time_column], format='%Y')\n",
    "        self.df = data.set_index(time_column)\n",
    "        self.time_column = time_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def optimize_arima(self, p_values, d_values, q_values):\n",
    "        \"\"\"\n",
    "        Optimize the ARIMA model by testing different combinations of (p, d, q).\n",
    "        \n",
    "        :param p_values: Range of p values to test (AR order).\n",
    "        :param d_values: Range of d values to test (Differencing order).\n",
    "        :param q_values: Range of q values to test (MA order).\n",
    "        \n",
    "        :return: Best fitted model and the best order (p, d, q).\n",
    "        \"\"\"\n",
    "        best_aic = np.inf\n",
    "        best_order = None\n",
    "        best_model = None\n",
    "\n",
    "        for p in p_values:\n",
    "            for d in d_values:\n",
    "                for q in q_values:\n",
    "                    try:\n",
    "                        model = ARIMA(self.df[self.value_column], order=(p, d, q))\n",
    "                        model_fit = model.fit()\n",
    "\n",
    "                        if model_fit.aic < best_aic:\n",
    "                            best_aic = model_fit.aic\n",
    "                            best_order = (p, d, q)\n",
    "                            best_model = model_fit\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        return best_model, best_order\n",
    "\n",
    "    def forecast(self, model, steps=6, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Forecast future values using the fitted ARIMA model.\n",
    "        \n",
    "        :param model: Fitted ARIMA model.\n",
    "        :param steps: Number of periods to forecast.\n",
    "        :param alpha: Significance level for confidence intervals (default is 0.05 for 95% interval).\n",
    "        \n",
    "        :return: Forecasted values and the corresponding confidence intervals.\n",
    "        \"\"\"\n",
    "        # help(model.forecast)\n",
    "        conf = model.get_forecast(steps=steps, alpha=alpha)\n",
    "        forecasts = model.forecast(steps=steps, alpha=alpha)\n",
    "\n",
    "        return forecasts, conf\n",
    "\n",
    "    def plot_forecast(self, forecast, conf_int, future_years):\n",
    "        \"\"\"\n",
    "        Plot the historical data, forecasted data, and confidence intervals.\n",
    "        \n",
    "        :param forecast: Forecasted values.\n",
    "        :param conf_int: Confidence intervals for the forecast.\n",
    "        :param future_years: Years corresponding to the forecasted values.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        prev = self.df.index.to_list()+future_years.to_list()\n",
    "        values = np.concatenate([self.df[self.value_column].to_numpy(), forecast])\n",
    "\n",
    "        # Historical data\n",
    "        plt.plot(prev, values, label=f'Historical {self.value_column.capitalize()}', color='blue', marker='o')\n",
    "\n",
    "        # Forecasted data\n",
    "        plt.plot(future_years, forecast, label=f'Forecasted {self.value_column.capitalize()}', color='red', linestyle='--', marker='x')\n",
    "        \n",
    "        # Plot confidence intervals\n",
    "        plt.fill_between(future_years, conf_int[:, 0], conf_int[:, 1], color='red', alpha=0.2, label='95% Prediction Interval')\n",
    "        \n",
    "        # Annotate forecasted values\n",
    "        for i, year in enumerate(future_years):\n",
    "            plt.text(year, forecast[i], f\"{forecast[i]:.2f}\", ha='center', color='red')\n",
    "            plt.text(year, forecast[i] + conf_int[i, 1] - forecast[i], f\"{forecast[i] + conf_int[i, 1] - forecast[i]:.2f}\", ha='center', color='red')\n",
    "            plt.text(year, forecast[i] - (conf_int[i, 1] - forecast[i]), f\"{forecast[i] - conf_int[i, 1] + forecast[i]:.2f}\", ha='center', color='red')\n",
    "\n",
    "\n",
    "        \n",
    "        # Labels and title\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(f'{self.value_column.capitalize()}')\n",
    "        plt.title(f'{self.value_column.capitalize()} Forecast with Prediction Intervals')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def model_summary(self, model):\n",
    "        \"\"\"\n",
    "        Print the summary of the fitted ARIMA model.\n",
    "        \n",
    "        :param model: The fitted ARIMA model.\n",
    "        \"\"\"\n",
    "        print(f\"Model Summary:\\n{model.summary()}\")\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        \"\"\"\n",
    "        Calculate and print the AIC of the fitted ARIMA model.\n",
    "        \n",
    "        :param model: The fitted ARIMA model.\n",
    "        \"\"\"\n",
    "        print(f\"AIC: {model.aic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "# Create an instance of the forecaster\n",
    "forecaster = TimeSeriesForecaster(acs1, time_column='year', value_column='Pop_Growth_Rate')\n",
    "\n",
    "# Step 1: Optimize ARIMA model (find the best (p, d, q) combination)\n",
    "p_values = range(0, 3)  # Test AR orders (p)\n",
    "d_values = range(0, 2)  # Test differencing orders (d)\n",
    "q_values = range(0, 3)  # Test MA orders (q)\n",
    "best_model, best_order = forecaster.optimize_arima(p_values, d_values, q_values)\n",
    "\n",
    "# Print best model and order\n",
    "print(f\"Best ARIMA model order: {best_order}\")\n",
    "\n",
    "# Step 2: Forecast future values\n",
    "steps = 3\n",
    "forecast, conf = forecaster.forecast(best_model, steps=steps)\n",
    "\n",
    "# Now generate the right number of future years\n",
    "last_year = acs1['year'].dt.year.max()\n",
    "future_years = pd.to_datetime(range(last_year + 1, last_year + 1 + steps), format='%Y')\n",
    "\n",
    "# Step 3: Plot the forecast with prediction intervals\n",
    "forecaster.plot_forecast(forecast.values, conf.conf_int().values, future_years)\n",
    "\n",
    "# Step 4: Print the model summary and AIC\n",
    "forecaster.model_summary(best_model)\n",
    "forecaster.evaluate_model(best_model)\n",
    "\n",
    "warnings.filterwarnings(\"default\")  # This line will make warnings visible again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "# Create an instance of the forecaster\n",
    "forecaster = TimeSeriesForecaster(acs1, time_column='year', value_column='Purchasing_Power')\n",
    "\n",
    "# Step 1: Optimize ARIMA model (find the best (p, d, q) combination)\n",
    "p_values = range(0, 3)  # Test AR orders (p)\n",
    "d_values = range(0, 2)  # Test differencing orders (d)\n",
    "q_values = range(0, 3)  # Test MA orders (q)\n",
    "best_model, best_order = forecaster.optimize_arima(p_values, d_values, q_values)\n",
    "\n",
    "# Print best model and order\n",
    "print(f\"Best ARIMA model order: {best_order}\")\n",
    "\n",
    "# Step 2: Forecast future values\n",
    "steps = 6\n",
    "forecast, conf = forecaster.forecast(best_model, steps=steps)\n",
    "\n",
    "# Now generate the right number of future years\n",
    "last_year = acs1['year'].dt.year.max()\n",
    "future_years = pd.to_datetime(range(last_year + 1, last_year + 1 + steps), format='%Y')\n",
    "\n",
    "# Step 3: Plot the forecast with prediction intervals\n",
    "forecaster.plot_forecast(forecast.values, conf.conf_int().values, future_years)\n",
    "\n",
    "# Step 4: Print the model summary and AIC\n",
    "forecaster.model_summary(best_model)\n",
    "forecaster.evaluate_model(best_model)\n",
    "\n",
    "warnings.filterwarnings(\"default\")  # This line will make warnings visible again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify competition or opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonemp_naics = nonemp\n",
    "cbp_naics = cbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonemp_naics = nonemp_naics.copy()\n",
    "nonemp_naics['year'] = pd.to_datetime(nonemp_naics['YEAR'], format='%Y')\n",
    "\n",
    "cbp_naics = cbp_naics.copy()\n",
    "cbp_naics['year'] = pd.to_datetime(cbp_naics['YEAR'], format='%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_merge_on_acs1 = list(set(nonemp_naics.columns)\n",
    "                        .intersection(set(cbp_naics.columns))\n",
    "                        .intersection(set(acs1.columns)))\n",
    "\n",
    "cols_to_merge_on = list(set(nonemp_naics.columns)\n",
    "                        .intersection(set(cbp_naics.columns)))\n",
    "\n",
    "print(cols_to_merge_on)\n",
    "print(cols_to_merge_on_acs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition = (acs1\n",
    "               .merge(nonemp_naics, on=cols_to_merge_on_acs1)\n",
    "               .merge(cbp_naics, on=cols_to_merge_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helps identify competition or opportunity.\n",
    "competition['Number_of_Businesses'] = competition['ESTAB']+competition['NESTAB']\n",
    "competition['Business_Density'] = competition['Number_of_Businesses']/competition['Population']*1000\n",
    "competition['Business_Density_ESTAB'] = competition['ESTAB']/competition['Population']*1000\n",
    "competition['Business_Density_NESTAB'] = competition['NESTAB']/competition['Population']*1000\n",
    "\n",
    "competition['expected_annual_revenue_NESTAB'] = (competition['NRCPTOT']/competition['NESTAB']).round(2)\n",
    "competition['expected_annual_payroll_ESTAB'] = (competition['PAYANN']/competition['ESTAB']).round(2)\n",
    "\n",
    "\n",
    "competition[['year','NAME','NAICS_LABEL','Number_of_Businesses','ESTAB','EMP','NESTAB',\n",
    "             'expected_annual_revenue_NESTAB', 'expected_annual_payroll_ESTAB',\n",
    "             'Business_Density','Business_Density_ESTAB','Business_Density_NESTAB']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From this analysis, we’ll identify: \n",
    "- **Business Saturation and Industry Concentration** – Analyzes industry saturation and identifies underserved sectors.\n",
    "- **Oversaturated sectors:** High establishment density, small firm size\n",
    "- **Underserved sectors:** Low density, large firm size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate establishments per 10,000 people\n",
    "competition['est_per_10k'] = (competition['ESTAB'] / competition['Population']) * 10000\n",
    "\n",
    "# Calculate average employees per establishment\n",
    "competition['emp_per_est'] = competition['EMP'] / competition['ESTAB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for industries with low saturation but high average size\n",
    "underserved = competition[(competition['est_per_10k'] < competition['est_per_10k'].quantile(0.25)) &\n",
    "                 (competition['emp_per_est'] > competition['emp_per_est'].quantile(0.75))]\n",
    "\n",
    "print(\"Underserved Industries:\")\n",
    "print(underserved[['NAICS', 'est_per_10k', 'emp_per_est']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=competition, x='est_per_10k', y='emp_per_est', hue='NAICS')\n",
    "plt.title('Industry Saturation vs. Employment per Establishment')\n",
    "plt.xlabel('Establishments per 10,000 People')\n",
    "plt.ylabel('Employees per Establishment')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "From this analysis, you’ll identify:\n",
    "- Oversaturated sectors: High establishment density, small firm size\n",
    "- Underserved sectors: Low density, large firm size\n",
    "\n",
    "**Advanced**: Location Quotient (LQ)\n",
    "\n",
    "Compare a region’s industry share to the national average.\n",
    "\n",
    "**Calculate** LQ = (Local share of industry / Local total) / (National share of industry / National total)\n",
    "\n",
    " You'd need national data too:\n",
    "1. Fetch national totals for each NAICS\n",
    "1. Calculate LQ for each sector\n",
    "\n",
    "**Pseudocode**:\n",
    "```LQ = (state_estab_i / state_total_estab) / (us_estab_i / us_total_estab)```\n",
    "\n",
    "**Interpretation**:\n",
    "- High ESTAB values → Possibly saturated sectors.\n",
    "- Low ESTAB with high demand → Opportunity for new entrants.\n",
    "- High LQ (>1.25) → Industry concentration above average (potential saturation).\n",
    "- Low LQ (<0.75) → Potentially underserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Next Steps:\n",
    "- Use geopandas to map this data geographically.\n",
    "- Add business data using sources like Yelp API or Google Places API for competitor analysis.\n",
    "- Use clustering (sklearn) to group similar counties/tracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather American Community Survey 5-Year Data (2012 - 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{acs5_params['dataset_base'].replace('/', '_')}_P1.csv\"\n",
    "if os.path.exists(acs5_file_path):\n",
    "    acs5_1 = pd.read_csv(acs5_file_path)\n",
    "else: \n",
    "    acs5_all_years = []\n",
    "    for i in range(2012, 2026):    \n",
    "        acs5_params['year'] = i\n",
    "        acs5_params['variables'] = \"NAME,B01003_001E,B19013_001E,B01002_001E,B15003_022E,B15003_023E,B15003_024E,B15003_025E,B01001_012E,B01001_011E,B01001_014E,B01001_013E,B01001_016E,B01001_015E,B01001_019E,B01001_017E,B01001_018E,B01001_020E,B01001_022E,B01001_021E,B01001_024E,B01001_023E,B01001_028E,B01001_029E,B01001_027E,B01001_030E,B01001_032E,B01001_031E,B01001_034E,B01001_033E,B01001_036E,B01001_035E,B01001_038E,B01001_037E,B01001_039E,B01001_040E,B01001_042E,B01001_041E,B01001_044E,B01001_043E,B01001_046E\"\n",
    "        try:\n",
    "            acs5 = pd.DataFrame(get_data(acs5_params))\n",
    "            acs5.columns = acs5.iloc[0]\n",
    "            acs5 = acs5.iloc[1:]     \n",
    "            acs5['year'] = i    \n",
    "            acs5_all_years.append(acs5)\n",
    "\n",
    "            print(f'finished year {i}')\n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "\n",
    "    acs5_1 = pd.concat(acs5_all_years)\n",
    "    acs5_1.to_csv(acs5_file_path, index=False)\n",
    "\n",
    "print(acs5_1.info())\n",
    "acs5_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{acs5_params['dataset_base'].replace('/', '_')}_P2.csv\"\n",
    "if os.path.exists(acs5_file_path):\n",
    "    acs5_2 = pd.read_csv(acs5_file_path)\n",
    "else: \n",
    "    acs5_all_years = []\n",
    "    for i in range(2012, 2026):    \n",
    "        acs5_params['year'] = i\n",
    "        acs5_params['variables'] = \"NAME,B01001_045E,B01001_048E,B01001_047E,B01001_004E,B01001_003E,B01001_006E,B01001_007E,B01001_005E,B01001_009E,B01001_008E,B01001_010E,B19083_001E,C02003_008E,C02003_004E,C02003_003E,C02003_007E,C02003_006E,C02003_005E,B11001_001E,B11001_002E,B11001_003E,B11001_004E,B11001_005E,B11001_007E,B15003_001E,B15003_017E,B15003_021E,C24010_001E,C24010_003E,C24010_004E,C24010_005E,C24010_006E,C24010_007E,C24010_014E,C24010_022E,C24010_027E,C24010_033E,C24010_042E,C24010_048E,C24010_050E,C17002_002E,C17002_003E\"\n",
    "        try:\n",
    "            acs5 = pd.DataFrame(get_data(acs5_params))\n",
    "            acs5.columns = acs5.iloc[0]\n",
    "            acs5 = acs5.iloc[1:]     \n",
    "            acs5['year'] = i    \n",
    "            acs5_all_years.append(acs5)\n",
    "\n",
    "            print(f'finished year {i}')\n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "\n",
    "    acs5_2 = pd.concat(acs5_all_years)\n",
    "    acs5_2.to_csv(acs5_file_path, index=False)\n",
    "\n",
    "print(acs5_2.info())\n",
    "acs5_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5_file_path = f\"data/{NAICS}_{COUNTYCODE}_{STATEFIPS}_{acs5_params['dataset_base'].replace('/', '_')}_P3.csv\"\n",
    "if os.path.exists(acs5_file_path):\n",
    "    acs5_3 = pd.read_csv(acs5_file_path)\n",
    "else: \n",
    "    acs5_all_years = []\n",
    "    for i in range(2012, 2026):    \n",
    "        acs5_params['year'] = i\n",
    "        acs5_params['variables'] = \"NAME,B25001_001E,B25002_002E,B25002_003E,B25003_001E,B25003_002E,B25003_003E,B25024_002E,B25024_003E,B25024_004E,B25024_005E,B25024_006E,B25024_007E,B25024_008E,B25024_009E,B25024_010E,B23025_005E,B23025_003E\"\n",
    "        try:\n",
    "            acs5 = pd.DataFrame(get_data(acs5_params))\n",
    "            acs5.columns = acs5.iloc[0]\n",
    "            acs5 = acs5.iloc[1:]     \n",
    "            acs5['year'] = i    \n",
    "            acs5_all_years.append(acs5)\n",
    "\n",
    "            print(f'finished year {i}')\n",
    "        except:\n",
    "            print(f'failed year {i}')\n",
    "\n",
    "    acs5_3 = pd.concat(acs5_all_years)\n",
    "    acs5_3.to_csv(acs5_file_path, index=False)\n",
    "\n",
    "print(acs5_3.info())\n",
    "acs5_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_acs_cols = list(set(acs5_1.columns).intersection(acs5_2))\n",
    "print(common_acs_cols)\n",
    "acs5 = (acs5_1\n",
    "        .merge(acs5_2, on=common_acs_cols)\n",
    "        .merge(acs5_3, on=common_acs_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5['NAME'] = acs5[['state', 'tract', 'county']].astype(str).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5[['state', 'tract', 'year', 'county','NAME']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Column Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_dictionary(f\"{base_url}/2022/{acs5_params['dataset_base']}/variables.json\", 'C24010_001E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_variables_in_data_dictionary(f\"{base_url}/2022/{acs5_params['dataset_base']}/variables.json\", \"years Sex by Age_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acs5.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5_cols_to_rename = {\n",
    "    'B01003_001E': 'Population',\n",
    "    'B19013_001E': 'Median_Income',\n",
    "    'B01002_001E': 'Median_Age',\n",
    "    'B15003_022E': 'Bachelors',\n",
    "    'B15003_023E': 'Masters',\n",
    "    'B15003_024E': 'Professional',\n",
    "    'B15003_025E': 'Doctorate',\n",
    "    'B01001_012E': 'M_30_34',\n",
    "    'B01001_011E': 'M_25_29',\n",
    "    'B01001_014E': 'M_40_44',\n",
    "    'B01001_013E': 'M_35_39',\n",
    "    'B01001_016E': 'M_50_54',\n",
    "    'B01001_015E': 'M_45_49',\n",
    "    'B01001_019E': 'M_62_64',\n",
    "    'B01001_017E': 'M_55_59',\n",
    "    'B01001_018E': 'M_60_61',\n",
    "    'B01001_020E': 'M_65_66',\n",
    "    'B01001_022E': 'M_70_74',\n",
    "    'B01001_021E': 'M_67_69',\n",
    "    'B01001_024E': 'M_80_84',\n",
    "    'B01001_023E': 'M_75_79',\n",
    "    'B01001_028E': 'F_5_9',\n",
    "    'B01001_029E': 'F_10_14',\n",
    "    'B01001_027E': 'F_0_5',\n",
    "    'B01001_030E': 'F_15_17',\n",
    "    'B01001_032E': 'F_20',\n",
    "    'B01001_031E': 'F_18_19',\n",
    "    'B01001_034E': 'F_22_24',\n",
    "    'B01001_033E': 'F_21',\n",
    "    'B01001_036E': 'F_30_34',\n",
    "    'B01001_035E': 'F_25_29',\n",
    "    'B01001_038E': 'F_40_44',\n",
    "    'B01001_037E': 'F_35_39',\n",
    "    'B01001_039E': 'F_45_49',\n",
    "    'B01001_040E': 'F_50_54',\n",
    "    'B01001_042E': 'F_60_61',\n",
    "    'B01001_041E': 'F_55_59',\n",
    "    'B01001_044E': 'F_65_66',\n",
    "    'B01001_043E': 'F_62_64',\n",
    "    'B01001_046E': 'F_70_74',\n",
    "    'B01001_045E': 'F_67_69',\n",
    "    'B01001_048E': 'F_80_84',\n",
    "    'B01001_047E': 'F_75_79',\n",
    "    'B01001_004E': 'M_5_9',\n",
    "    'B01001_003E': 'M_0_5',\n",
    "    'B01001_006E': 'M_15_17',\n",
    "    'B01001_007E': 'M_18_19',\n",
    "    'B01001_005E': 'M_10_14',\n",
    "    'B01001_009E': 'M_21',\n",
    "    'B01001_008E': 'M_20',\n",
    "    'B01001_010E': 'M_22_24',\n",
    "    'B19083_001E': 'Gini_Index',\n",
    "    'C02003_008E': 'Other',\n",
    "    'C02003_004E': 'Black',\n",
    "    'C02003_003E': 'White',\n",
    "    'C02003_007E': 'Native_Hawaiian_and_Other_Pacific_Islander',\n",
    "    'C02003_006E': 'Asian',\n",
    "    'C02003_005E': 'American_Indian_and_Alaska_Native',\n",
    "    'B11001_001E': 'Total Households',\n",
    "    'B11001_002E': 'Family Households',\n",
    "    'B11001_003E': 'Married Couple Families',\n",
    "    'B11001_004E': 'Male Householder No Wife',\n",
    "    'B11001_005E': 'Female Householder No Husband',\n",
    "    'B11001_007E': 'Nonfamily Households',\n",
    "    'B15003_001E': 'Total',\n",
    "    'B15003_017E': 'High School',\n",
    "    'B15003_021E': \"Associate's\",\n",
    "    'C24010_001E': 'Total Employed',\n",
    "    'C24010_003E': 'Management',\n",
    "    'C24010_004E': 'Business',\n",
    "    'C24010_005E': 'Computer',\n",
    "    'C24010_006E': 'Engineering',\n",
    "    'C24010_007E': 'Science',\n",
    "    'C24010_014E': 'Healthcare',\n",
    "    'C24010_022E': 'Protective Service',\n",
    "    'C24010_027E': 'Sales',\n",
    "    'C24010_033E': 'Office Support',\n",
    "    'C24010_042E': 'Construction',\n",
    "    'C24010_048E': 'Production',\n",
    "    'C24010_050E': 'Transportation',\n",
    "    \"B25001_001E\": \"Total_Housing_Units\",\n",
    "    \"B25002_002E\": \"Occupied_Units\",\n",
    "    \"B25002_003E\": \"Vacant_Units\",\n",
    "    \"B25003_001E\": \"Total_Tenure\",\n",
    "    \"B25003_002E\": \"Owner_Occupied\",\n",
    "    \"B25003_003E\": \"Renter_Occupied\",\n",
    "    \"B25024_002E\": \"1-Unit_Detached\",\n",
    "    \"B25024_003E\": \"1-Unit_Attached\",\n",
    "    \"B25024_004E\": \"2_Units\",\n",
    "    \"B25024_005E\": \"3_4_Units\",\n",
    "    \"B25024_006E\": \"5_9_Units\",\n",
    "    \"B25024_007E\": \"10_19_Units\",\n",
    "    \"B25024_008E\": \"20+_Units\",\n",
    "    \"B25024_009E\": \"Mobile_Homes\",\n",
    "    \"B25024_010E\": \"Other_Housing\",\n",
    "    'B23025_005E': 'Unemployed',\n",
    "    'B23025_003E': 'Labor_force'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_range_parsed = [int(val) for val in target_market['demographics']['age_range'].split('–')]\n",
    "age_range = range(age_range_parsed[0], age_range_parsed[1])\n",
    "age_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_columns = {k: v for k, v in acs5_cols_to_rename.items() \\\n",
    "               if ('B01001' in k) \\\n",
    "               and (int(v.split(\"_\")[1:][0]) in age_range) \\\n",
    "                and (int(v.split(\"_\")[1:][-1]) in age_range)}\n",
    "age_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_M_columns = {k: v for k, v in age_columns.items() if ('M' in v)}\n",
    "age_M_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_F_columns = {k: v for k, v in age_columns.items() if ('F' in v)}\n",
    "age_F_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "acs5.rename(columns=acs5_cols_to_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will have the follwoing in the next cells\n",
    "* **Age Cohort Analysis** – Identifies dominant age groups to tailor services.\n",
    "* **Language & Cultural Composition Mapping** – Analyzes diversity for localized strategies.\n",
    "* **Poverty and Assistance Needs Analysis** – Identifies areas with economic disadvantages.\n",
    "* **Family and Household Composition** – Assesses family structures and sizes.\n",
    "* **Educational Attainment & Skills Assessment** – Analyzes education levels and workforce skills.\n",
    "* **Housing Characteristics Review** – Examines housing types, ownership, and vacancy data.\n",
    "\n",
    "Also after checking\n",
    "- Set Median Income > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in acs5.columns:\n",
    "    try:\n",
    "        acs5[col] = acs5[col].astype(float)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5 = acs5[acs5['Median_Income']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5 = acs5.copy()\n",
    "acs5['Unemployment_Rate'] = acs5['Unemployed'] / acs5['Labor_force'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and calculate averages (if you have multiple entries per year)\n",
    "trend_data = acs5.groupby('year').agg({\n",
    "    'Median_Income': 'mean',\n",
    "    'Unemployment_Rate': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plot both trends\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(data=trend_data, x='year', y='Median_Income', ax=ax1, color='blue', label='Median Income')\n",
    "ax1.set_ylabel('Median Income ($)', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Add second axis for unemployment rate\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=trend_data, x='year', y='Unemployment_Rate', ax=ax2, color='red', label='Unemployment Rate')\n",
    "ax2.set_ylabel('Unemployment Rate (%)', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title('Median Income and Unemployment Rate Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "# Create an instance of the forecaster\n",
    "forecaster = TimeSeriesForecaster(trend_data, time_column='year', value_column='Median_Income')\n",
    "\n",
    "# Step 1: Optimize ARIMA model (find the best (p, d, q) combination)\n",
    "p_values = range(0, 3)  # Test AR orders (p)\n",
    "d_values = range(0, 2)  # Test differencing orders (d)\n",
    "q_values = range(0, 3)  # Test MA orders (q)\n",
    "best_model, best_order = forecaster.optimize_arima(p_values, d_values, q_values)\n",
    "\n",
    "# Print best model and order\n",
    "print(f\"Best ARIMA model order: {best_order}\")\n",
    "\n",
    "# Step 2: Forecast future values\n",
    "steps = 6\n",
    "forecast, conf = forecaster.forecast(best_model, steps=steps)\n",
    "\n",
    "# Now generate the right number of future years\n",
    "last_year = acs1['year'].dt.year.max()\n",
    "future_years = pd.to_datetime(range(last_year + 1, last_year + 1 + steps), format='%Y')\n",
    "\n",
    "# Step 3: Plot the forecast with prediction intervals\n",
    "forecaster.plot_forecast(forecast.values, conf.conf_int().values, future_years)\n",
    "\n",
    "# Step 4: Print the model summary and AIC\n",
    "forecaster.model_summary(best_model)\n",
    "forecaster.evaluate_model(best_model)\n",
    "\n",
    "warnings.filterwarnings(\"default\")  # This line will make warnings visible again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "# Create an instance of the forecaster\n",
    "forecaster = TimeSeriesForecaster(trend_data, time_column='year', value_column='Unemployment_Rate')\n",
    "\n",
    "# Step 1: Optimize ARIMA model (find the best (p, d, q) combination)\n",
    "p_values = range(0, 3)  # Test AR orders (p)\n",
    "d_values = range(0, 2)  # Test differencing orders (d)\n",
    "q_values = range(0, 3)  # Test MA orders (q)\n",
    "best_model, best_order = forecaster.optimize_arima(p_values, d_values, q_values)\n",
    "\n",
    "# Print best model and order\n",
    "print(f\"Best ARIMA model order: {best_order}\")\n",
    "\n",
    "# Step 2: Forecast future values\n",
    "steps = 6\n",
    "forecast, conf = forecaster.forecast(best_model, steps=steps)\n",
    "\n",
    "# Now generate the right number of future years\n",
    "last_year = acs1['year'].dt.year.max()\n",
    "future_years = pd.to_datetime(range(last_year + 1, last_year + 1 + steps), format='%Y')\n",
    "\n",
    "# Step 3: Plot the forecast with prediction intervals\n",
    "forecaster.plot_forecast(forecast.values, conf.conf_int().values, future_years)\n",
    "\n",
    "# Step 4: Print the model summary and AIC\n",
    "forecaster.model_summary(best_model)\n",
    "forecaster.evaluate_model(best_model)\n",
    "\n",
    "warnings.filterwarnings(\"default\")  # This line will make warnings visible again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calculations\n",
    "acs5 = acs5.copy()\n",
    "acs5[\"Vacancy_Rate (%)\"] = 100 * acs5[\"Vacant_Units\"] / acs5[\"Total_Housing_Units\"]\n",
    "acs5[\"Owner_Rate (%)\"] = 100 * acs5[\"Owner_Occupied\"] / acs5[\"Total_Tenure\"]\n",
    "acs5[\"Renter_Rate (%)\"] = 100 * acs5[\"Renter_Occupied\"] / acs5[\"Total_Tenure\"]\n",
    "\n",
    "# Group by year and calculate the mean for each group\n",
    "acs5_yearly = acs5.groupby(\"year\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Plot Vacancy, Owner, and Renter Rates over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(acs5_yearly[\"year\"], acs5_yearly[\"Vacancy_Rate (%)\"], label=\"Vacancy Rate (%)\", marker='o')\n",
    "plt.plot(acs5_yearly[\"year\"], acs5_yearly[\"Owner_Rate (%)\"], label=\"Owner-Occupied Rate (%)\", marker='s')\n",
    "plt.plot(acs5_yearly[\"year\"], acs5_yearly[\"Renter_Rate (%)\"], label=\"Renter-Occupied Rate (%)\", marker='^')\n",
    "\n",
    "plt.title(\"Housing Occupancy Rates Over Time (Aggregated by year)\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of housing type columns\n",
    "housing_type_cols = [\n",
    "    \"1-Unit_Detached\", \"1-Unit_Attached\", \"2_Units\", \"3_4_Units\",\n",
    "    \"5_9_Units\", \"10_19_Units\", \"20+_Units\", \"Mobile_Homes\", \"Other_Housing\"\n",
    "]\n",
    "\n",
    "# Calculate percentage columns\n",
    "for col in housing_type_cols:\n",
    "    acs5[col + \"_Pct\"] = 100 * acs5[col] / acs5[\"Total_Housing_Units\"]\n",
    "\n",
    "# Aggregate by year (mean across geographies)\n",
    "grouped = acs5.groupby(\"year\")[[col + \"_Pct\" for col in housing_type_cols]].mean()\n",
    "\n",
    "# Rename columns for readability\n",
    "grouped.columns = [col.replace(\"_Pct\", \"\").replace(\"_\", \" \") for col in grouped.columns]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in grouped.columns:\n",
    "    plt.plot(grouped.index, grouped[col], label=col, marker='o')\n",
    "\n",
    "plt.title(\"Average Housing Type Percentages by year\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"Percentage of Total Housing Units\")\n",
    "plt.legend(title=\"Housing Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped aggregation by year\n",
    "acs5['HighEd'] = acs5[[\"Bachelors\", \"Masters\", 'Professional', 'Doctorate']].sum(axis=1)\n",
    "acs5['HighEd_pct'] = (acs5['HighEd'] / acs5['Total']) * 100\n",
    "\n",
    "# Skill mapping\n",
    "skill_map = {\n",
    "    'High Skill': ['Management', 'Business', 'Computer', 'Engineering', 'Science', 'Healthcare'],\n",
    "    'Medium Skill': ['Sales', 'Office Support', 'Protective Service', 'Construction'],\n",
    "    'Low Skill': ['Production', 'Transportation']\n",
    "}\n",
    "\n",
    "# Prepare lists for plotting\n",
    "years = []\n",
    "education_rates = []\n",
    "high_skill_rates = []\n",
    "gaps = []\n",
    "\n",
    "# Loop through each year to calculate metrics\n",
    "for year, group in acs5.groupby('year'):\n",
    "    years.append(year)\n",
    "\n",
    "    total_emp = group['Total Employed'].values[0]\n",
    "    high_ed_pct = group['HighEd_pct'].values[0]\n",
    "    high_skill_count = sum(group[j].values[0] for j in skill_map['High Skill'])\n",
    "    high_skill_pct = (high_skill_count / total_emp) * 100\n",
    "\n",
    "    education_rates.append(high_ed_pct)\n",
    "    high_skill_rates.append(high_skill_pct)\n",
    "    gaps.append(high_ed_pct - high_skill_pct)\n",
    "\n",
    "# Plot the line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(years, education_rates, marker='o', label='Higher Education Rate')\n",
    "plt.plot(years, high_skill_rates, marker='s', label='High Skill Job Rate')\n",
    "plt.plot(years, gaps, marker='^', label='Education-Job Gap')\n",
    "\n",
    "plt.title('Education vs. High-Skill Employment Over Time (California)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages\n",
    "acs5['% Family Households'] = 100 * acs5['Family Households'] / acs5['Total Households']\n",
    "acs5['% Nonfamily Households'] = 100 * acs5['Nonfamily Households'] / acs5['Total Households']\n",
    "\n",
    "# Plot family vs non-family households by state\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_sorted = acs5.sort_values('% Family Households', ascending=False).head(20)\n",
    "sns.barplot(x='NAME', y='% Family Households', data=df_sorted, color='skyblue', label='Family')\n",
    "sns.barplot(x='NAME', y='% Nonfamily Households', data=df_sorted, color='orange', label='Nonfamily')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Family vs Nonfamily Households by Tract Samples (ACS 2021)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_acs5_cols = list(acs5_cols_to_rename.values())\n",
    "acs5[new_acs5_cols] = acs5[new_acs5_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs5 = acs5.copy()\n",
    "\n",
    "# Convert numerical columns\n",
    "acs5['Edu_High'] = acs5['Bachelors'] + acs5['Masters'] + acs5['Professional'] + acs5['Doctorate']\n",
    "acs5['Edu_Rate'] = (acs5['Edu_High'] / acs5['Population']) * 100\n",
    "\n",
    "age_range_values = '_'.join(str(val) for val in age_range_parsed)\n",
    "age_range_column = f'Age_{age_range_values}'\n",
    "age_range_pct_column = age_range_column+'_Pct'\n",
    "\n",
    "M_age_range_column = f'M_Age_{age_range_values}'\n",
    "M_age_range_pct_column = M_age_range_column+'_Pct'\n",
    "\n",
    "F_age_range_column = f'F_Age_{age_range_values}'\n",
    "F_age_range_pct_column = F_age_range_column+'_Pct'\n",
    "\n",
    "# Example: % of people aged 25–44 (prime working/spending age)\n",
    "acs5[age_range_column] = acs5[age_columns.values()].sum(axis=1)\n",
    "acs5[age_range_pct_column] = (acs5[age_range_column] / acs5['Population']) * 100\n",
    "\n",
    "acs5[M_age_range_column] = acs5[age_M_columns.values()].sum(axis=1)\n",
    "acs5[M_age_range_pct_column] = (acs5[M_age_range_column] / acs5['Population']) * 100\n",
    "\n",
    "acs5[F_age_range_column] = acs5[age_F_columns.values()].sum(axis=1)\n",
    "acs5[F_age_range_pct_column] = (acs5[F_age_range_column] / acs5['Population']) * 100\n",
    "\n",
    "# Simpson's Diversity Index example:\n",
    "ethnic_cols = ['Other', 'Black', 'White', \n",
    "               'Native_Hawaiian_and_Other_Pacific_Islander', \n",
    "               'Asian', 'American_Indian_and_Alaska_Native']\n",
    "\n",
    "ethnic_shares = acs5[ethnic_cols].div(acs5['Population'], axis=0)\n",
    "acs5['Diversity_Index'] = 1 - (ethnic_shares ** 2).sum(axis=1)\n",
    "\n",
    "# Step 1: Select Features\n",
    "features = ['Population', 'Median_Income', 'Edu_Rate', 'Diversity_Index', age_range_column]\n",
    "\n",
    "# Step 2: Drop rows with missing values (if any)\n",
    "acs5_clean = acs5\n",
    "\n",
    "# Step 3: Normalize the features (0–1 scale)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(acs5_clean[features])\n",
    "acs5_scaled = pd.DataFrame(scaled_features, columns=features, index=acs5_clean.index)\n",
    "\n",
    "# Step 4: Apply PCA to determine weights\n",
    "pca = PCA(n_components=1)  # We only want the 1st principal component\n",
    "pca.fit(acs5_scaled)\n",
    "\n",
    "# Get PCA loadings (importance of each variable in PC1)\n",
    "# PCA provides data-driven weights based on the underlying variance structure of the data\n",
    "\n",
    "# We created a composite score using five community metrics. Based on data patterns, income and \n",
    "# education explain the majority of differences between areas, suggesting these should be primary \n",
    "# focus areas for resource allocation. Demographic diversity, while important, plays a smaller \n",
    "# statistical role in differentiation within this dataset.\n",
    "\n",
    "loadings = np.abs(pca.components_[0])\n",
    "weights = loadings / loadings.sum()  # Normalize to sum to 1\n",
    "\n",
    "# Optional: print weights for interpretability\n",
    "weights_df = pd.DataFrame({'Feature': features, 'Weight': weights})\n",
    "print(\"PCA-derived weights:\\n\", weights_df)\n",
    "\n",
    "# Step 5: Compute weighted score (socio-economic index)\n",
    "acs5_clean['Score'] = acs5_scaled.dot(weights)\n",
    "\n",
    "# Optional: Merge back with original dataset if rows were dropped\n",
    "acs5.loc[acs5_clean.index, 'Score'] = acs5_clean['Score']\n",
    "\n",
    "\n",
    "# Caculate poverty rate\n",
    "acs5[['C17002_002E', 'C17002_003E']] = acs5[['C17002_002E', 'C17002_003E']].astype(float)\n",
    "acs5[\"Poverty_Rate\"] = (acs5[\"C17002_002E\"] + acs5[\"C17002_003E\"]) / acs5[\"Population\"] * 100\n",
    "\n",
    "display_columns = ['NAME', 'Population', 'Median_Income', 'Gini_Index', \n",
    "                   'Edu_High', 'Edu_Rate', 'Poverty_Rate', 'Median_Age', \n",
    "                   'Diversity_Index', age_range_pct_column, M_age_range_pct_column, \n",
    "                   F_age_range_pct_column, 'Score', 'year']\n",
    "\n",
    "# Sort by Median Income (for example)\n",
    "top_tracts = acs5[acs5['year'] == 2023].sort_values(by='Median_Income', ascending=False).head(10)\n",
    "print('display top 10 tracts in a county with highest median income')\n",
    "display(top_tracts[display_columns])\n",
    "\n",
    "# Sort by Median Income (for example)\n",
    "top_tracts = acs5[acs5['year'] == 2023].sort_values(by='Median_Income', ascending=True).head(10)\n",
    "print('display top 10 tracts in a county with Lowest median income')\n",
    "display(top_tracts[display_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Customer Segmentation** – Breaks down customers into targetable segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Review this and make it readable!!!\n",
    "target_market.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_market['demographics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = acs5[display_columns]\n",
    "current_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = current_data.copy()\n",
    "current_data['year'] = pd.to_datetime(current_data['year'], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Population', 'Median_Income', 'Gini_Index', 'Edu_High',\n",
    "       'Edu_Rate', 'Poverty_Rate', 'Median_Age', 'Diversity_Index',\n",
    "       'Age_18_55_Pct', 'M_Age_18_55_Pct', 'F_Age_18_55_Pct', 'Score']\n",
    "\n",
    "current_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_timeseries_rows(X):\n",
    "    \"\"\"\n",
    "    Linearly interpolate missing values (NaNs) row-wise for 2D time series data.\n",
    "    \n",
    "    Parameters:\n",
    "        X (np.ndarray): 2D NumPy array with shape (n_samples, n_timestamps)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Interpolated array of the same shape\n",
    "    \"\"\"\n",
    "    interpolated = np.empty_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        row = pd.Series(X.iloc[i])\n",
    "        interpolated[i] = row.interpolate(method='linear', limit_direction='both').values\n",
    "    \n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = [interpolate_timeseries_rows(current_data.pivot(index='NAME', columns='year', values=feat)) for feat in features]\n",
    "\n",
    "X = np.stack([p for p in pivoted], axis=-1)\n",
    "\n",
    "# Normalize across time and stores per feature\n",
    "n_samples, n_time, n_features = X.shape\n",
    "X_reshaped = X.reshape(-1, n_features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled.reshape(n_samples, n_time, n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "model = TimeSeriesKMeans(n_clusters=4, metric=\"softdtw\", random_state=42)\n",
    "labels = model.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_tracts = current_data.pivot(index='NAME', columns='year', values=features[0]).index\n",
    "\n",
    "# Add labels back to stores\n",
    "df_labels = pd.DataFrame({'census_tracts': census_tracts, 'Cluster': labels})\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = features\n",
    "\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for cluster_id, centroid in enumerate(model.cluster_centers_):\n",
    "        plt.plot(centroid[:, i], label=f'Cluster {cluster_id}')\n",
    "    plt.title(f'Cluster Centroids for {feature_name}')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(feature_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_flat = X_scaled.reshape(n_samples, -1)\n",
    "tsne = TSNE(n_components=2, perplexity=10, random_state=0, max_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_flat)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette=\"Set2\", s=100)\n",
    "for i, store in enumerate(df_labels):\n",
    "    plt.text(X_tsne[i, 0]+0.3, X_tsne[i, 1], store, fontsize=8)\n",
    "plt.title(\"t-SNE Projection of Census Tract Time Series (Clustered)\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['state_tract_county'] = gdf[['STATEFP', 'TRACTCE', 'COUNTYFP']].astype(int).astype(str).sum(axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GIS Mapping & Visualization** – Uses geographic data to visualize key factors in map format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the cluster labels with the geographic data\n",
    "gdf_clusters = gdf.merge(df_labels, how='inner', left_on='state_tract_county', right_on='census_tracts')\n",
    "\n",
    "# Plot the geographic distribution of clusters\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "gdf_clusters.plot(column='Cluster', categorical=True, cmap='Set2', legend=True, ax=ax, edgecolor='black', linewidth=0.4)\n",
    "\n",
    "# Optional: Add labels to tracts\n",
    "for idx, row in gdf_clusters.iterrows():\n",
    "    plt.annotate(text=row['TRACTCE'], xy=(row.geometry.centroid.x, row.geometry.centroid.y),\n",
    "                 horizontalalignment='center', fontsize=2)\n",
    "\n",
    "plt.title(\"Geographic Distribution of Clusters by Census Tract\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Next Steps:\n",
    "- Use geopandas to map this data geographically.\n",
    "- Add business data using sources like Yelp API or Google Places API for competitor analysis.\n",
    "- Use clustering (sklearn) to group similar counties/tracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate and Compare:  \n",
    "- **Visit Potential Locations:** Conduct site visits to assess the suitability of each location firsthand\n",
    "- **Cost Analysis:** Compare the costs of different \n",
    "    - locations\n",
    "    - including rent\n",
    "    - utilities\n",
    "    - taxes  \n",
    "- **Infrastructure:** Assess the availability of essential infrastructure, such as \n",
    "    - transportation\n",
    "    - utilities\n",
    "    - internet access\n",
    "- **Pros and Cons:** Create a list of pros and cons for each potential location to help make a decision\n",
    "- **Long-Term Growth Potential:** Consider the long-term growth potential of the area and its ability to support your business  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consider Legal and Institutional Factors: \n",
    "- **Business Licenses:** Research the necessary business licenses and permits required for your business type and location. \n",
    "- **Local Regulations:** Understand any local regulations or restrictions that may affect your business operations.\n",
    "- **Government Incentives:** Explore any local or state government incentives or programs that may be available for businesses in specific areas.\n",
    "- **Zoning Laws:** Understand local zoning regulations and restrictions to ensure compliance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
